{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, Flatten\n",
    "\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123\n",
    "gamma=0.99\n",
    "max_steps_per_episode=1000\n",
    "env=gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(CartPole-v0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.17472449, -0.98461736, -1.5274704 ]),\n",
       " -1.8011789172338466,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=4\n",
    "num_actions=2\n",
    "num_hidden=128\n",
    "\n",
    "inputs=Input((num_inputs,))\n",
    "hidden=Dense(num_hidden,activation='relu')(inputs)\n",
    "action=Dense(num_actions,activation='softmax')(hidden)\n",
    "critic=Dense(1)(hidden)\n",
    "model=Model(inputs=inputs,outputs=[action,critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.122754373]]\n"
     ]
    }
   ],
   "source": [
    "tf.print(action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 80.25 at episode 10\n",
      "running reward: 128.30 at episode 20\n",
      "running reward: 157.07 at episode 30\n",
      "running reward: 174.30 at episode 40\n",
      "running reward: 184.61 at episode 50\n",
      "running reward: 190.79 at episode 60\n",
      "running reward: 194.48 at episode 70\n",
      "Solved at episode 72!\n"
     ]
    }
   ],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss=keras.losses.Huber()\n",
    "action_probs_history=[]\n",
    "critic_value_history=[]\n",
    "rewards_history=[]\n",
    "running_reward=0\n",
    "episode_count=0\n",
    "\n",
    "while True:\n",
    "    state=env.reset()\n",
    "    episode_reward=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1,max_steps_per_episode):\n",
    "            state=tf.convert_to_tensor(state)\n",
    "            state=tf.expand_dims(state,0)\n",
    "            \n",
    "            action_probs,critic_value=model(state)\n",
    "            critic_value_history.append(critic_value[0,0])\n",
    "            \n",
    "            \n",
    "            action=np.random.choice(num_actions,p=np.squeeze(action_probs))\n",
    "            \n",
    "            action_probs_history.append(tf.math.log(action_probs[0,action]))\n",
    "            \n",
    "            env.render()\n",
    "            state,reward,done,info=env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward+=reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        running_reward=0.05*episode_reward+(1-0.05)*running_reward\n",
    "        \n",
    "        returns=[]\n",
    "        discounted_sum=0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum=r+gamma*discounted_sum\n",
    "            returns.insert(0,discounted_sum)\n",
    "        \n",
    "        returns=np.array(returns)\n",
    "        returns=(returns-np.mean(returns))/(np.std(returns)+eps)\n",
    "        returns=returns.tolist()\n",
    "        history=zip(action_probs_history,critic_value_history,returns)\n",
    "        actor_losses=[]\n",
    "        critic_losses=[]\n",
    "        for log_prob,value,ret in history:\n",
    "            diff=ret-value\n",
    "            actor_losses.append(-log_prob*diff)\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value,0),tf.expand_dims(ret,0)))\n",
    "            \n",
    "        losses_value=sum(actor_losses)+sum(critic_losses)\n",
    "        grads=tape.gradient(losses_value,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "    \n",
    "    episode_count+=1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
